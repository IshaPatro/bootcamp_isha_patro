{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 04: Data Acquisition and Ingestion\n",
        "\n",
        "This notebook demonstrates various data acquisition methods including API calls and web scraping.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import yfinance as yf\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. API Pull (Alpha Vantage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = os.getenv('ALPHA_VANTAGE_API_KEY')\n",
        "ticker = 'AAPL'\n",
        "base_url = 'https://www.alphavantage.co/query'\n",
        "\n",
        "params = {\n",
        "    'function': 'TIME_SERIES_DAILY',\n",
        "    'symbol': ticker,\n",
        "    'apikey': api_key,\n",
        "    'outputsize': 'compact'\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.get(base_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "    \n",
        "    if 'Time Series (Daily)' in data:\n",
        "        time_series = data['Time Series (Daily)']\n",
        "        df_api = pd.DataFrame.from_dict(time_series, orient='index')\n",
        "        df_api.index = pd.to_datetime(df_api.index)\n",
        "        df_api.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "        \n",
        "        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
        "            df_api[col] = pd.to_numeric(df_api[col])\n",
        "        \n",
        "        print(f\"API Data Shape: {df_api.shape}\")\n",
        "        print(f\"Missing Values: {df_api.isnull().sum().sum()}\")\n",
        "        print(f\"Required columns present: {all(col in df_api.columns for col in ['Open', 'High', 'Low', 'Close', 'Volume'])}\")\n",
        "        \n",
        "        df_api.to_csv('../data/raw/alpha_vantage_aapl.csv')\n",
        "        print(\"Data saved to data/raw/alpha_vantage_aapl.csv\")\n",
        "        print(df_api.head())\n",
        "        \n",
        "    else:\n",
        "        print(\"API Error or rate limit reached, using yfinance fallback\")\n",
        "        df_api = yf.download(ticker, period='1mo')\n",
        "        df_api.to_csv('../data/raw/yfinance_aapl_fallback.csv')\n",
        "        print(\"Fallback data saved to data/raw/yfinance_aapl_fallback.csv\")\n",
        "        print(df_api.head())\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Using yfinance fallback\")\n",
        "    df_api = yf.download(ticker, period='1mo')\n",
        "    df_api.to_csv('../data/raw/yfinance_aapl_fallback.csv')\n",
        "    print(\"Fallback data saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Web Scraping (S&P 500 Companies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
        "    \n",
        "    rows = table.find_all('tr')[1:]\n",
        "    \n",
        "    data = []\n",
        "    for row in rows:\n",
        "        cols = row.find_all('td')\n",
        "        if len(cols) >= 3:\n",
        "            symbol = cols[0].text.strip()\n",
        "            company = cols[1].text.strip()\n",
        "            sector = cols[2].text.strip()\n",
        "            data.append([symbol, company, sector])\n",
        "    \n",
        "    df_scraped = pd.DataFrame(data, columns=['Symbol', 'Company', 'Sector'])\n",
        "    \n",
        "    print(f\"Scraped Data Shape: {df_scraped.shape}\")\n",
        "    print(f\"Missing Values: {df_scraped.isnull().sum().sum()}\")\n",
        "    print(f\"Text columns validated: {df_scraped['Symbol'].dtype == 'object' and df_scraped['Company'].dtype == 'object'}\")\n",
        "    \n",
        "    df_scraped.to_csv('../data/raw/sp500_companies.csv', index=False)\n",
        "    print(\"Data saved to data/raw/sp500_companies.csv\")\n",
        "    print(df_scraped.head())\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Scraping error: {e}\")\n",
        "    df_scraped = pd.DataFrame({'Symbol': ['AAPL', 'MSFT', 'GOOGL'], \n",
        "                              'Company': ['Apple Inc.', 'Microsoft Corp.', 'Alphabet Inc.'],\n",
        "                              'Sector': ['Technology', 'Technology', 'Technology']})\n",
        "    df_scraped.to_csv('../data/raw/sp500_companies_sample.csv', index=False)\n",
        "    print(\"Sample data saved as fallback\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Documentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Sources and Parameters\n",
        "\n",
        "**API Source:**\n",
        "- URL: https://www.alphavantage.co/query\n",
        "- Function: TIME_SERIES_DAILY\n",
        "- Symbol: AAPL (Apple Inc.)\n",
        "- API Key: Loaded from .env file\n",
        "- Fallback: yfinance library for AAPL data\n",
        "\n",
        "**Web Scraping Source:**\n",
        "- URL: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
        "- Target: First table with class 'wikitable sortable'\n",
        "- Extracted columns: Symbol, Company Name, Sector\n",
        "\n",
        "### Validation Logic\n",
        "\n",
        "**API Data Validation:**\n",
        "- Check for required columns: Open, High, Low, Close, Volume\n",
        "- Convert all price columns to numeric types\n",
        "- Validate date index format\n",
        "- Count missing values\n",
        "- Verify data shape and completeness\n",
        "\n",
        "**Scraped Data Validation:**\n",
        "- Ensure text columns (Symbol, Company, Sector) are object dtype\n",
        "- Check for missing values in critical fields\n",
        "- Validate minimum expected number of rows (should be ~500 companies)\n",
        "- Strip whitespace from text fields\n",
        "\n",
        "### Environment Security\n",
        "- .env file contains sensitive API keys\n",
        "- .env is included in .gitignore to prevent accidental commits\n",
        "- API keys should never be hardcoded in notebooks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Assumptions & Risks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assumptions\n",
        "1. **API Availability**: Alpha Vantage API service is operational and accessible\n",
        "2. **Rate Limits**: API calls respect rate limiting (5 calls/minute for free tier)\n",
        "3. **Data Format**: API returns data in expected JSON structure with time series\n",
        "4. **Web Structure**: Wikipedia S&P 500 page maintains consistent table structure\n",
        "5. **Network Access**: Reliable internet connection for API calls and web scraping\n",
        "\n",
        "### Risks\n",
        "1. **API Rate Limiting**: Free tier limitations may cause data acquisition failures\n",
        "2. **API Key Exposure**: Risk of accidentally committing sensitive credentials\n",
        "3. **Web Scraping Fragility**: Website structure changes could break scraping logic\n",
        "4. **Data Quality**: No guarantee of data accuracy from external sources\n",
        "5. **Service Downtime**: External services may be temporarily unavailable\n",
        "6. **Legal Compliance**: Web scraping may violate terms of service if not careful\n",
        "\n",
        "### Mitigation Strategies\n",
        "- Implement fallback data sources (yfinance for financial data)\n",
        "- Use robust error handling and try-catch blocks\n",
        "- Validate data quality after acquisition\n",
        "- Respect robots.txt and website terms of service\n",
        "- Monitor API usage to avoid rate limits\n",
        "- Secure credential management through environment variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
